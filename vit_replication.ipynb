{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gauravreddy08/pytorch-vision-transformer/blob/main/vit_replication.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SV0euaTIJiD-"
      },
      "source": [
        "# Replicating **Vision Transformer** using **PyTorch**\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/gauravreddy08/pytorch-vision-transformer/main/assets/architecture.png\" alt=\"Architecture \" width=\"550\">\n",
        "\n",
        "> **Vision Transformer Paper:** [**An image is worth 16x16 words**](https://arxiv.org/abs/2010.11929)\n",
        "\n",
        "> **Original Transformer Paper:** [**Attention is all you need**](https://arxiv.org/abs/1706.03762)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KO_z4DzBNf_o"
      },
      "outputs": [],
      "source": [
        "!pip install -q torchinfo wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rahadUEJcoC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import wandb\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torchvision\n",
        "from torchinfo import summary\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "  torch.manual_seed(42)\n",
        "  torch.cuda.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jz4EMVFqOZq4"
      },
      "outputs": [],
      "source": [
        "# Self Made helper_functions to ease our model development and training process\n",
        "# Visit : https://github.com/gauravreddy08/learning-pytorch/tree/main/going_modular\n",
        "\n",
        "!git clone https://github.com/gauravreddy08/learning-pytorch\n",
        "!mv /content/learning-pytorch/going_modular .\n",
        "!rm -rf learning-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9T4bhGNN0IN"
      },
      "outputs": [],
      "source": [
        "# Initiating WandB project \n",
        "run = wandb.init(project=\"vit-replication\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading the **dataset**\n",
        "\n",
        "> **Source:** [`github.com/mrdbourke/pytorch-deep-learning/pizza_steak_sushi.zip`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/data/pizza_steak_sushi.zip)"
      ],
      "metadata": {
        "id": "w6-_AVmR3h29"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5jy_DDLOB12"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "from pathlib import Path\n",
        "import requests\n",
        "\n",
        "def download_data(source: str,\n",
        "                  destination: str,\n",
        "                  remove_source: bool = True) -> Path:\n",
        "    data = Path(\"data/\")\n",
        "    data_path = data /destination\n",
        "    if(data_path.is_dir()):\n",
        "      print(\"[INFO] Data already exists...\")\n",
        "    else:\n",
        "      print(f\"[INFO] Did not find {data_path} directory, creating one...\")\n",
        "      data_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "      target_file = Path(source).name\n",
        "      with open(data/target_file, 'wb') as f:\n",
        "        request = requests.get(source)\n",
        "        print(f\"[INFO] Downloading {target_file} from {source}...\")\n",
        "        f.write(request.content)\n",
        "      with zipfile.ZipFile(data/target_file, 'r') as zip_ref:\n",
        "        print(f\"[INFO] Unzipping {target_file} data...\")\n",
        "        zip_ref.extractall(data_path)\n",
        "      if remove_source:\n",
        "        os.remove(data/target_file)\n",
        "\n",
        "    return data_path\n",
        "\n",
        "data_path = download_data(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\", \n",
        "                          destination='pizaa_steak_sushi')\n",
        "\n",
        "train_dir = data_path/'train'\n",
        "test_dir = data_path/'test'\n",
        "\n",
        "print(f\"Training Directory: {train_dir}\")\n",
        "print(f\"Testing Directory: {test_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtZirRffPbdU"
      },
      "source": [
        "## Turning **data** into **dataloaders**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rT-p5oFYOnbJ"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "IMG_SIZE = (224, 224) # Table 3\n",
        "BATCH_SIZE = 32 # paper used 4096, we don't have computation power\n",
        "\n",
        "# Creating transforms\n",
        "manual_transforms = transforms.Compose([\n",
        "    transforms.Resize(IMG_SIZE),\n",
        "    transforms.ToTensor()])\n",
        "\n",
        "manual_transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ESi4Z8zPZFL"
      },
      "outputs": [],
      "source": [
        "from going_modular import data_setup\n",
        "\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataset(\n",
        "    train_dir=train_dir,\n",
        "    test_dir=test_dir,\n",
        "    transform=manual_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kBVLahyP3n7"
      },
      "outputs": [],
      "source": [
        "imgs, labels = next(iter(train_dataloader))\n",
        "image, label = imgs[0], labels[0]\n",
        "\n",
        "print(f\"Image Shape: {image.shape}\")\n",
        "print(f\"Example Label: {label}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jz0ePjdmQQFU"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(7, 7))\n",
        "\n",
        "plt.imshow(image.permute(1, 2, 0))\n",
        "plt.title(f\"Label: {class_names[label]}\")\n",
        "plt.axis('off');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vw8jkzLhjstv"
      },
      "source": [
        "## Building the **ViT** Model \n",
        "\n",
        "### **Part I: Input to our Architecture**\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/gauravreddy08/pytorch-vision-transformer/main/assets/architecture.png\" alt=\"Architecture \" width=\"550\">\n",
        "\n",
        "#### **Decoding the Architecture:** \n",
        "The **ViT architecture** is heavily inspired by the Transformer architecture (hence the name **\"Vision Transformer\"**). Instead of looking at the data as a whole, these networks focus and give \"attention\" to specific parts of data or an image, in our case. In a **Vision Transformer (ViT)**, the input image is first divided into smaller patches, and each patch is treated like a word in a sentence. The patches are then processed by a *transformer encoder* that's been adapted for image analysis.\n",
        "\n",
        "So as an initial input through our network, images are resized and cropped into several different **patches**. These patches are then passed through the *transformer encoder* as **1D Tensors**. Also, each patch (image) is coupled with its respective **\"positional embedding\"**. And there's an extra learnable **\"classification token [CLS]\"** prepended in the sequence of embeddings.\n",
        "\n",
        "> **What should be the target size of the input image?**\n",
        "> \n",
        "> The target size of the input image should be 224x224. \n",
        "> > Refer table [4](https://arxiv.org/pdf/2010.11929.pdf#page=5).\n",
        "\n",
        "> **What number of patches are extracted from a image? `3x3`? `5x5`? `10x10`?**\n",
        ">\n",
        "> The Base-ViT model, also commonly known as ViT16, utilizes 16x16 sized patches resulting in a total of 196 patches. \n",
        "> > Refer page [5](https://arxiv.org/pdf/2010.11929.pdf#page=5). \n",
        "``` \n",
        " 14 * 14 = 196 + 1 (CLS token) = 197\n",
        "```\n",
        "\n",
        "> **What should the dimension of the then transformed 1D Tensors be?**\n",
        "> \n",
        "> The dimension of the transformed 1D Tensors should be 768. Not so coincidentally, when a patch image (3 x 16 x 16) is flattened and reshaped into a 1D tensor, we end up with a 768 dimension tensor.\n",
        ">\n",
        "> ```python\n",
        "> [16 x 16 x 3] -> nn.Flatten() -> [1 x 768]\n",
        "> ```\n",
        ">\n",
        "> > Refer Table [1](https://arxiv.org/pdf/2010.11929.pdf#page=5).\n",
        "\n",
        "> **What exact layers are used to generate the image embeddings?**\n",
        ">\n",
        "> ViT uses **Conv2D layers** for both generating embeddings while also dividing the input image into patches. \n",
        ">\n",
        "> You may ask, how can we *patchify* the image using conv layers ? \n",
        ">\n",
        "> The Conv2D layers patchify the image using kernel size and stride. Specifically, setting the `kernel_size` and `stride` to 16, we can parse over the image as 16 by 16 blocks and generate the embeddings. More about this can be found in the code.\n",
        "\n",
        "> **What's the extra learnable \"classification token\" mentioned?**\n",
        ">\n",
        "> The **Class Token** is randomly initialized and doesn’t contain any useful information on its own. However, the **Class Token** accumulates information from the other tokens in the sequence the deeper and more layers the Transformer is. \n",
        "\n",
        "> When the Vision Transformer finally performs the final classification of the sequence, it uses an MLP head which only looks at data from the last layer’s Class Token and no other information. [`source`](https://deepganteam.medium.com/vision-transformers-for-computer-vision-9f70418fe41a)\n",
        "\n",
        "> **What's a positional embedding?**\n",
        "> \n",
        "> The position of a patch relative to the whole image proves to be crucial information. For example, the whitespace in \"the rapist\" and \"therapist.\" Transformers, unlike LSTMs, take inputs in parallel and all at once, which means they lose information about the sequence of these patches. To avoid this, we couple each image embedding with its respective positional embedding. [`source`](https://www.youtube.com/watch?v=dichIcUZfOw&ab_channel=HeduAI)\n",
        "\n",
        "\n",
        "$$\\mathbf{z}_0=\\left[\\mathbf{x}_{\\text {class }} ; \\mathbf{x}_p^1 \\mathbf{E} ; \\mathbf{x}_p^2 \\mathbf{E} ; \\cdots ; \\mathbf{x}_p^N \\mathbf{E}\\right]+\\mathbf{E}_{p o s}, \\quad \\mathbf{E} \\in \\mathbb{R}^{\\left(P^2 \\cdot C\\right) \\times D}, \\mathbf{E}_{p o s} \\in \\mathbb{R}^{(N+1) \\times D} $$\n",
        "\n",
        "Equation 1 pretty much sums up the whole above content into a mathematical form. \n",
        "\n",
        "But, how about a psuedo code ? \n",
        "\n",
        "**Equation 1 Psuedo Code :**\n",
        "```python\n",
        "x_input = [class_token, patch_1, patch_2, ..., patch_N] + [pos_0, pos_1, pos_2, ..., pos_N]\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQu8nqQuRmps"
      },
      "outputs": [],
      "source": [
        "width = 224\n",
        "height = 224\n",
        "channels = 3\n",
        "\n",
        "patch_size = 16\n",
        "patch_count = int((width*height)/(patch_size**2))\n",
        "print(f\"Number of Patches (N) = {patch_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dlk9Pnwzqps2"
      },
      "source": [
        "Interesting... The number of patches in a **224 x 224** image is **196**.\n",
        "\n",
        "Now let's convert them into a 1D Tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsXxIvG6q5i-"
      },
      "outputs": [],
      "source": [
        "dummy_patch = torch.randn(patch_count, 16, 16, 3)\n",
        "dummy_patch.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXmAVOu2qNPf"
      },
      "outputs": [],
      "source": [
        "final_one_d_tensor = nn.Flatten()(dummy_patch)\n",
        "print(f\"Final 1D Tensors Size: {final_one_d_tensor.shape} -> (number of patches, embedding size)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualising a **image** as **patches**"
      ],
      "metadata": {
        "id": "q0VCPvNTEyA8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0nmbH8oraJ6"
      },
      "outputs": [],
      "source": [
        "image_permuted = image.permute(1, 2, 0)\n",
        "image_permuted.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMC5p6Cbgw8w"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "plt.imshow(image_permuted)\n",
        "plt.axis('off');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xz5WaXQsNouT"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 2))\n",
        "plt.imshow(image_permuted[:patch_size, :, :])\n",
        "plt.axis('off');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kwLdqu9NhJ-"
      },
      "outputs": [],
      "source": [
        "img_size = 224\n",
        "\n",
        "fig, axs = plt.subplots(224//patch_size, 224//patch_size, \n",
        "                        figsize = (12, 12), sharex=True, sharey=True)\n",
        "for j, patch_h in enumerate(range(0, img_size, patch_size)):\n",
        "  for i, patch in enumerate(range(0, img_size, patch_size)):\n",
        "    axs[j, i].imshow(image_permuted[patch_h:patch_h+patch_size, \n",
        "                                    patch:patch+patch_size, \n",
        "                                    :])\n",
        "    axs[j, i].axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJd-FKNKvXCr"
      },
      "source": [
        "### Converting **images** into **patches** with `nn.Conv2D` layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OG_ILq-3R_LI"
      },
      "outputs": [],
      "source": [
        "patch_embedding = nn.Conv2d(in_channels=3, out_channels=768, # 16*16*3\n",
        "                   kernel_size=16, stride=16, padding=0)\n",
        "patch_embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JaeLmXJrw1R6"
      },
      "outputs": [],
      "source": [
        "image.unsqueeze(dim=0).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbJAYnk5VgBv"
      },
      "outputs": [],
      "source": [
        "patch_embedding.eval()\n",
        "\n",
        "with torch.inference_mode():\n",
        "  pred = patch_embedding((image.unsqueeze(dim=0)))\n",
        "\n",
        "print(f\"Shape of patched image: {pred.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Converting **patches** into **1D tensors** with `nn.Flatten()` layer"
      ],
      "metadata": {
        "id": "4RnupaXeHtmR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERA4d0LswMHb"
      },
      "outputs": [],
      "source": [
        "print(f\"After flattening: {nn.Flatten(start_dim=2, end_dim=3)(pred).shape}\")\n",
        "print(f\"Adjusting the output shape: {nn.Flatten(start_dim=2, end_dim=3)(pred).permute(0, 2, 1).shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combining everything into a **python** reusable class "
      ],
      "metadata": {
        "id": "0qUKPNujXF5U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EocgrrJc10ML"
      },
      "outputs": [],
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "  def __init__(self, in_channels: int = 3,\n",
        "               out_channels: int = 768,\n",
        "               patch_size: int = 16):\n",
        "    super().__init__()\n",
        "    self.patch_embedding = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
        "                   kernel_size=patch_size, stride=patch_size, padding=0),\n",
        "        nn.Flatten(start_dim=2, end_dim=3))\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    embedding = self.patch_embedding(x)\n",
        "    return embedding.permute(0, 2, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJ_JQTm66YgY"
      },
      "outputs": [],
      "source": [
        "dummy_tensor_batch = torch.randn(32, 3, 224, 224)\n",
        "patchify = PatchEmbedding()\n",
        "\n",
        "print(f\"Input image batch shape: {dummy_tensor_batch.shape}\")\n",
        "generated_embedding = patchify(dummy_tensor_batch)\n",
        "print(f\"Output image batch shape: {generated_embedding.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introducing learnable **[CLS] token**\n",
        "\n",
        "The **[CLS] token** is considered to be unimportant as it does not carry any significant information. Instead, it is initialized as a random number collection. However, it serves as a **placeholder** for data, which is then utilized by the head of the architecture for image classification.\n",
        "\n",
        "To represent an entire image for classification purposes, a **[CLS] token** is incorporated. Additionally, the authors introduce absolute position embeddings and process the resultant sequence of vectors through a conventional Transformer encoder."
      ],
      "metadata": {
        "id": "iHR4X-y4XfFz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPfbzN0E6vMF"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "\n",
        "class_token = nn.Parameter(torch.rand(batch_size, 1, 768), \n",
        "                           requires_grad=True)\n",
        "class_token.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XJWEFTC9dp2"
      },
      "outputs": [],
      "source": [
        "# Prepending class token to rest series of patches\n",
        "torch.cat((class_token, generated_embedding), \n",
        "          dim=1).shape "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introducing **Positional Embeddings**\n",
        "\n",
        "By incorporating **positional embeddings**, the model gains valuable information about the location of each patch which can be used for further analysis. \n",
        "\n",
        "> **How are these position embeddings generated ?**\n",
        ">\n",
        "> The process of generating positional embeddings is not as straightforward as assigning *sequential numbers* (such as 1, 2, 3, ..., N) to each patch. Instead, a combination of **sine** and **cosine** functions is used to create a series of embeddings that encode the position of each patch. For more information on the specifics of this process. More info here. "
      ],
      "metadata": {
        "id": "78IgKK_bY1_q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mslgfzZZ948q"
      },
      "outputs": [],
      "source": [
        "positional_embedding = torch.Tensor()\n",
        "\n",
        "for single_patch in list(range(196)):  \n",
        "  vector = torch.Tensor()\n",
        "  for i in list(range(0, 768, 2)):\n",
        "    sine_val = torch.sin(torch.Tensor([single_patch/(10000**((2*i)/768))]))\n",
        "    cosine_val = torch.cos(torch.Tensor([single_patch/(10000**((2*(i+1))/768))]))\n",
        "    \n",
        "    vector = torch.cat((vector, sine_val, cosine_val))\n",
        "  positional_embedding = torch.cat((positional_embedding, vector.unsqueeze(dim=0)))\n",
        "\n",
        "positional_embedding, positional_embedding.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combining everything into a single **`PatchEmbedding` class**"
      ],
      "metadata": {
        "id": "EhArTQwPZbBZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLfzGMPljFnF"
      },
      "outputs": [],
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "  def __init__(self, in_channels: int = 3,\n",
        "               out_channels: int = 768,\n",
        "               patch_size: int = 16,\n",
        "               image_size: int = 224, \n",
        "               device: torch.device = device):\n",
        "    super().__init__()\n",
        "    self.out_channels = out_channels\n",
        "\n",
        "    ## Seperating images into patches\n",
        "    self.patch_embedding = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
        "                   kernel_size=patch_size, stride=patch_size, padding=0),\n",
        "        nn.Flatten(start_dim=2, end_dim=3)).to(device)\n",
        "\n",
        "    ## Generating Positional Embeddings\n",
        "    self.positional_embedding = torch.Tensor()\n",
        "    \n",
        "    for single_patch in list(range((image_size**2//patch_size**2)+1)):  \n",
        "      vector = torch.Tensor()\n",
        "      for i in list(range(0, out_channels, 2)):\n",
        "        sine_val = torch.sin(torch.Tensor([single_patch/(10000**((2*i)/out_channels))]))\n",
        "        cosine_val = torch.cos(torch.Tensor([single_patch/(10000**((2*(i+1))/out_channels))]))\n",
        "        \n",
        "        vector = torch.cat((vector, sine_val, cosine_val))\n",
        "      self.positional_embedding = torch.cat((self.positional_embedding, vector.unsqueeze(dim=0)))\n",
        "    \n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    embedding = self.patch_embedding(x) # Patchify\n",
        "\n",
        "    self.batch_size = embedding.shape[0] \n",
        "    self.cls_token = nn.Parameter(torch.randn(self.batch_size, 1,\n",
        "                                              self.out_channels), requires_grad=True).to(device) # Creating a [CLS] token\n",
        "\n",
        "    embedding = torch.cat((self.cls_token, embedding.permute(0, 2, 1)), dim=1) # Prepending the [CLS] token\n",
        "    embedding = embedding + self.positional_embedding.to(device) # Adding positional embeddings\n",
        "\n",
        "    return embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVDgBcHzukBs"
      },
      "outputs": [],
      "source": [
        "dummy_tensor_batch = torch.randn(32, 3, 224, 224).to(device) # Dummy Tensor\n",
        "patchify = PatchEmbedding()\n",
        "\n",
        "print(f\"Input image batch shape: {dummy_tensor_batch.shape}\")\n",
        "generated_embedding = patchify(dummy_tensor_batch)\n",
        "print(f\"Output image batch shape: {generated_embedding.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Looks like our input is ready to go throught the network. \n",
        ">\n",
        "> **But, what does our network consists ?**"
      ],
      "metadata": {
        "id": "pyIBMQyFamLr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6796mfdSY-l"
      },
      "source": [
        "### **Part II: The Transformer Encoder**\n",
        "\n",
        "After converting our images into 1D tensors, as explained in the paper, we can now pass these tensors through the **Transformer Encoder**. The encoder comprises multiple layers, each containing two distinct *ConvBlocks* known as the **MSA Block** and **MLP Block**.\n",
        "\n",
        "> **How many layers should our encoder contain ?**\n",
        "> \n",
        "> **12**. Refer Table [1](https://arxiv.org/pdf/2010.11929.pdf#page=5)\n",
        "\n",
        "#### 1. **MultiHead Self-Attention (MSA)** block\n",
        "\n",
        "This convolutional block comprises of two layers - **MSA** and **LayerNorm**, as shown in the equation below.\n",
        "\n",
        "Assuming $\\mathbf{z}_{\\ell-1}$ is passed through the LayerNorm layer first, then through the MSA layer, the resulting output is $\\mathbf{z}_{\\ell}^{\\prime}$. Additionally, the output is combined with the original input $\\mathbf{z}_{\\ell-1}$, which acts as a residual block.\n",
        "\n",
        "$$\\mathbf{z}_{\\ell}^{\\prime}=\\operatorname{MSA}\\left(\\operatorname{LN}\\left(\\mathbf{z}_{\\ell-1}\\right)\\right)+\\mathbf{z}_{\\ell-1}, \\quad \\ell=1 \\ldots L$$\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/gauravreddy08/pytorch-vision-transformer/main/assets/architecture.png\" alt=\"Architecture \" width=\"500\"> </center>\n",
        "\n",
        "> **What is a MultiHead Self-Attention ?**\n",
        "> \n",
        "> Watch three part series by . [**`1/SelfAttention`**](https://www.youtube.com/watch?v=yGTUuEx3GkA&t=640s&ab_channel=Rasa), [**`2/Keys,Values,Queries`**](https://www.youtube.com/watch?v=tIvKXrEDMhk&ab_channel=Rasa) & [**`3/MultiHeadAttention`**](https://www.youtube.com/watch?v=23XUv0T9L5c&ab_channel=Rasa)\n",
        "\n",
        "> **What is the purpose of adding initial inputs back again ?**\n",
        "> \n",
        "> This is known as Residual blocks. This method is used in a very popular model, guess what? **ResNets!!**\n",
        "> \n",
        "> Watch [**`ResNet (actually) explained in under 10 minutes`**](https://www.youtube.com/watch?v=o_3mboe1jYI&ab_channel=rupertai)\n",
        "> \n",
        "> Also, [**`Vanishing & Exploding Gradient explained`**](https://www.youtube.com/watch?v=qO_NLVjD6zE&ab_channel=deeplizard)\n",
        "\n",
        "**Equation 2 Psuedo Code :**\n",
        "```python\n",
        "msa_output = MSA(x_input) + x_input\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfYA1a1-SYkU"
      },
      "outputs": [],
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "  def __init__(self,\n",
        "               embedding_dim: int = 768,\n",
        "               num_heads : int = 12,\n",
        "               attn_dropout : int = 0):\n",
        "    super().__init__()\n",
        "\n",
        "    self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
        "\n",
        "    self.multi_attn = nn.MultiheadAttention(\n",
        "        embed_dim = embedding_dim,\n",
        "        num_heads = num_heads, batch_first = True) # batch_first -> (batch, seq, features) -> (32, 197, 768)\n",
        "\n",
        "  def forward(self, x):\n",
        "        x = self.layer_norm(x)\n",
        "        attn_output, _ = self.multi_attn(query=x, key=x,value=x, \n",
        "                                         need_weights=False) \n",
        "        return attn_output    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5nSZE7mw32B"
      },
      "outputs": [],
      "source": [
        "# Create an instance of MSABlock\n",
        "multihead_self_attention_block = MultiHeadSelfAttention(embedding_dim=768, # from Table 1 \n",
        "                                                        num_heads=12).to(device) # from Table 1\n",
        "\n",
        "# Pass patch and position image embedding through MSABlock\n",
        "patched_image_through_msa_block = multihead_self_attention_block(generated_embedding)\n",
        "print(f\"Input shape of MSA block: {generated_embedding.shape}\")\n",
        "print(f\"Output shape MSA block: {patched_image_through_msa_block.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_22GFz_m33yM"
      },
      "source": [
        "#### 2. **MultiLayer Perceptron (MLP)** block\n",
        "\n",
        "Although the term **\"perceptron\"** may seem complex, it is actually a very simple concept that you may already be familiar with. Let's break it down *word* by *word*. The word **\"perceptron\"** refers to a **neuron**, which in the context of artificial intelligence (AI), is simply a **dense layer**. In PyTorch, a **dense layer** is represented by the **`nn.Linear` layer**. A **\"multi-layered\"** perceptron simply means stacking multiple **`nn.Linear` layers** on top of each other. It's important to note that activation functions such as **GeLU** and **Dropout layers** are added in between these **`nn.Linear` layers**.\n",
        "\n",
        "Similar to the previous block, this block follows the same pipeline by adding the initial input to the output produced by the **MLP block**, which acts as a residual block.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/gauravreddy08/pytorch-vision-transformer/main/assets/architecture.png\" alt=\"Architecture \" width=\"500\"> </center>\n",
        "\n",
        "$$\\mathbf{z}_{\\ell}=\\operatorname{MLP}\\left(\\operatorname{LN}\\left(\\mathbf{z}_{\\ell}^{\\prime}\\right)\\right)+\\mathbf{z}_{\\ell}^{\\prime}, \\quad \\ell=1 \\ldots L$$\n",
        "\n",
        "<!-- <img src=\"https://raw.githubusercontent.com/gauravreddy08/pytorch-vision-transformer/main/assets/architecture.png\" alt=\"Architecture \" width=\"500\"> -->\n",
        "\n",
        "> **Why do we add Dropout layers ?**\n",
        "> \n",
        "> **Reason 1:** Cause the paper says so. Refer section [ ***B.1 Training***](https://arxiv.org/pdf/2010.11929.pdf#page=13)\n",
        ">\n",
        "> >  \"*Dropout, when used, is applied after every dense layer except forthe qkv-projections and directly after adding positional to patch embeddings*\"\n",
        ">\n",
        "> **Reason 2:** Dropout layers are a regularization technique used in neural networks to prevent overfitting. Overfitting occurs when a neural network becomes too complex and is too closely fit to the training data, resulting in poor performance when applied to new, unseen data.\n",
        "\n",
        "> **What does GeLU activation do ?**\n",
        "> \n",
        "> **GeLU** stands for **Gaussian Error Linear Units**. It is an activation function that was proposed as an alternative to other commonly used activation functions, such as **ReLU (Rectified Linear Unit)** and **Sigmoid** functions.\n",
        ">\n",
        "> The **GeLU** activation function is a smooth function that is similar to the **Sigmoid** function, but with some advantages. It is a continuous, differentiable, and monotonic function that can be efficiently computed.\n",
        "\n",
        "**Equation 3 Psuedo Code :**\n",
        "```python\n",
        "mlp_output = MLP(msa_output) + msa_output\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edm4yMmE3nTt"
      },
      "outputs": [],
      "source": [
        "class MLPBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "                 embedding_dim:int=768, # Hidden Size D from Table 1 for ViT-Base\n",
        "                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n",
        "                 dropout:float=0.1): # Dropout from Table 3 for ViT-Base\n",
        "        super().__init__()\n",
        "      \n",
        "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
        "        \n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(in_features=embedding_dim,\n",
        "                      out_features=mlp_size),\n",
        "            nn.GELU(), # \"The MLP contains two layers with a GELU non-linearity (section 3.1).\"\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(in_features=mlp_size,\n",
        "                      out_features=embedding_dim), \n",
        "            nn.Dropout(p=dropout) # \"Dropout, when used, is applied after every dense layer..\" she said\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.layer_norm(x)\n",
        "        x = self.mlp(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOMhGWmG4MNF"
      },
      "outputs": [],
      "source": [
        "# Create an instance of MLPBlock\n",
        "mlp_block = MLPBlock(embedding_dim=768, # from Table 1 \n",
        "                     mlp_size=3072, # from Table 1\n",
        "                     dropout=0.1).to(device) # from Table 3\n",
        "\n",
        "# Pass output of MSABlock through MLPBlock\n",
        "patched_image_through_mlp_block = mlp_block(patched_image_through_msa_block)\n",
        "print(f\"Input shape of MLP block: {patched_image_through_mlp_block.shape}\")\n",
        "print(f\"Output shape MLP block: {patched_image_through_mlp_block.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VafhT5TUvZan"
      },
      "source": [
        "Now stack these both blocks together...\n",
        "\n",
        "**Psuedo Code :**\n",
        "```python\n",
        "x_input -> MSA_block -> [MSA_block_output + x_input] -> MLP_block -> [MLP_block_output + MSA_block_output + x_input] -> ...\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSTU42gYvY4c"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoderBlock(nn.Module):\n",
        "  def __init__(self, embedding_dim : int = 768,\n",
        "               num_head : int = 12,\n",
        "               mlp_size : int = 3072,\n",
        "               mlp_dropout : int = 0.1,\n",
        "               attn_dropout : int = 0):\n",
        "    super().__init__()\n",
        "\n",
        "    self.msa_block = MultiHeadSelfAttention(embedding_dim=768, \n",
        "                                                        num_heads=12)\n",
        "\n",
        "    self.mlp_block = MLPBlock(embedding_dim=768, \n",
        "                     mlp_size=3072, \n",
        "                     dropout=0.1)\n",
        "  def forward(self, x):\n",
        "      x = self.msa_block(x) + x\n",
        "      x = self.mlp_block(x) + x\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mj2z208_4SvF"
      },
      "outputs": [],
      "source": [
        "tranformer_encoder = TransformerEncoderBlock().to(device)\n",
        "\n",
        "summary(tranformer_encoder, input_size = (1, 197, 768),\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Completing the **ViT** Architecture\n",
        "\n",
        "Up until this point, we have only built individual components of the ViT model. However, it is now time to integrate all of these parts to create the complete ViT model."
      ],
      "metadata": {
        "id": "FZ56TGQwZafF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzP2SmUNyHo1"
      },
      "outputs": [],
      "source": [
        "class ViT(nn.Module):\n",
        "  def __init__(self,\n",
        "               img_size=224, emb_size=768,\n",
        "               in_channels=3, patch_size=16,\n",
        "               num_layers=12, mlp_size=3072, \n",
        "               num_heads=12, attn_droput=0,\n",
        "               mlp_dropout=0.1, emb_dropout=0.1,\n",
        "               num_classes=1000, device=device):\n",
        "    \n",
        "    super().__init__()\n",
        "    \n",
        "    # 1. Input: Patch Embeddings\n",
        "    self.patch_embedding = PatchEmbedding() \n",
        "\n",
        "    # 2. Encoder: Multi (12) Layer Encoder\n",
        "    self.encoder = nn.Sequential(\n",
        "        *[TransformerEncoderBlock() for _ in range(num_layers)]\n",
        "    ).to(device) # creates multiple layers of the encoder, we need \"12\" for the ViT-base\n",
        "\n",
        "    # 2. Classification Head\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.LayerNorm(normalized_shape=emb_size),\n",
        "        nn.Linear(in_features = emb_size, out_features = num_classes)\n",
        "    ).to(device)\n",
        "\n",
        "  def forward(self, x : torch.Tensor):\n",
        "    x = (self.patch_embedding(x)).to(device)\n",
        "    x = self.encoder(x).to(device)\n",
        "\n",
        "    # Only using the [CLS] token for final classification\n",
        "    x = self.classifier(x[:, 0]).to(device) \n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6c0yViK9Z3d"
      },
      "outputs": [],
      "source": [
        "vit = ViT(num_classes = len(class_names)).to(device) # num_class = 3 -> [pizza, steak, sushi]\n",
        "\n",
        "# Print a summary of our custom ViT model using torchinfo\n",
        "summary(model=vit, \n",
        "        input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width)\n",
        "        col_names=[\"input_size\", \"output_size\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"], device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training** our model\n",
        "\n",
        "#### **HyperParameters :**\n",
        "* **Loss:** *Categorical Cross Entropy* Loss\n",
        " \n",
        "* **Optimizer:** *Adam* optimizer \n",
        "\n",
        " > *We train all models, including ResNets, using Adam (Kingma & Ba,\n",
        "2015) with β1 = 0.9, β2 = 0.999, a batch size of 4096 and apply a high weight decay of 0.1...* **- [Page 5, Training & Fine-tuning](https://arxiv.org/pdf/2010.11929.pdf#page=5)**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BxYrZoPGcvYT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-twIke5qICO4"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params=vit.parameters(), lr=1e-3,\n",
        "                             betas=(0.9, 0.999), weight_decay=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4NvbKuMIaok"
      },
      "outputs": [],
      "source": [
        "from going_modular import engine\n",
        "\n",
        "# Set the seeds\n",
        "set_seed() \n",
        "\n",
        "# Train the model and save the training results to a dictionary\n",
        "results = engine.train(model=vit,\n",
        "                       train_data=train_dataloader,\n",
        "                       test_data=test_dataloader,\n",
        "                       optimizer=optimizer,\n",
        "                       loss=loss_fn,\n",
        "                       epochs=10,\n",
        "                       device=device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = torch.tensor(results[\"train_loss\"])\n",
        "test_loss = torch.tensor(results[\"test_loss\"])\n",
        "\n",
        "accuracy = torch.tensor(results[\"train_acc\"])\n",
        "test_accuracy = torch.tensor(results[\"test_acc\"])\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(results['epochs'], loss, label='Training')\n",
        "plt.plot(results['epochs'], test_loss, label='Testing')\n",
        "plt.title(\"Loss\")\n",
        "plt.xlabel('epochs')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(results['epochs'], accuracy, label='Training')\n",
        "plt.plot(results['epochs'], test_accuracy, label='Testing')\n",
        "plt.title(\"Accuracy\")\n",
        "plt.xlabel('epochs')\n",
        "plt.legend();"
      ],
      "metadata": {
        "id": "3Zn6qKIQmyW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Wow.. Intresting...\n",
        "#### A *good* news and a *bad* news. \n",
        "**Good News:** Our model actually works. The one we made from almost scratch. It works! \n",
        "\n",
        "**Bad News:** But why isn't our model training well? ViT is meant to be competing with *state-of-the-art* models right?\n"
      ],
      "metadata": {
        "id": "K8hrPEeflVM9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trying out a **PreTrained ViT** model"
      ],
      "metadata": {
        "id": "muUD-I4Dl9K1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1dVyF7ZUoBf"
      },
      "outputs": [],
      "source": [
        "# 1. Get pretrained weights for ViT-Base\n",
        "pretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT # requires torchvision >= 0.13, \"DEFAULT\" means best available\n",
        "\n",
        "# 2. Setup a ViT model instance with pretrained weights\n",
        "pretrained_vit = torchvision.models.vit_b_16(weights=pretrained_vit_weights).to(device)\n",
        "\n",
        "# 3. Freeze the base parameters\n",
        "for parameter in pretrained_vit.parameters():\n",
        "    parameter.requires_grad = False\n",
        "    \n",
        "# 4. Change the classifier head (set the seeds to ensure same initialization with linear head)\n",
        "set_seed()\n",
        "pretrained_vit.heads = nn.Linear(in_features=768, out_features=len(class_names)).to(device)\n",
        "# pretrained_vit # uncomment for model output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying the transformers recommened by the \"pretrained\" model\n",
        "vit16_transforms = pretrained_vit_weights.transforms()\n",
        "\n",
        "# Loading a new pair of dataloaders\n",
        "new_train_dataloader, new_test_dataloader, class_names = data_setup.create_dataset(\n",
        "    train_dir=train_dir,\n",
        "    test_dir=test_dir,\n",
        "    transform=vit16_transforms)"
      ],
      "metadata": {
        "id": "E0Z5l8z4wbL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular import engine\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params=pretrained_vit.parameters(), lr=1e-3,\n",
        "                             betas=(0.9, 0.999), weight_decay=0.1)\n",
        " \n",
        "set_seed() # Sets random seed to \"42\"\n",
        "\n",
        "pretrained_results = engine.train(pretrained_vit, \n",
        "                                  epochs=10,\n",
        "                                  train_data=new_train_dataloader,\n",
        "                                  test_data=new_test_dataloader,\n",
        "                                  loss=loss_fn,\n",
        "                                  optimizer=optimizer,\n",
        "                                  device=device)"
      ],
      "metadata": {
        "id": "KHKuwk8qwwSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = torch.tensor(pretrained_results[\"train_loss\"])\n",
        "test_loss = torch.tensor(pretrained_results[\"test_loss\"])\n",
        "\n",
        "accuracy = torch.tensor(pretrained_results[\"train_acc\"])\n",
        "test_accuracy = torch.tensor(pretrained_results[\"test_acc\"])\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(pretrained_results['epochs'], loss, label='Training')\n",
        "plt.plot(pretrained_results['epochs'], test_loss, label='Testing')\n",
        "plt.title(\"Loss\")\n",
        "plt.xlabel('epochs')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(pretrained_results['epochs'], accuracy, label='Training')\n",
        "plt.plot(pretrained_results['epochs'], test_accuracy, label='Testing')\n",
        "plt.title(\"Accuracy\")\n",
        "plt.xlabel('epochs')\n",
        "plt.legend();"
      ],
      "metadata": {
        "id": "CrpqmA7ZnAaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_preds = []\n",
        "\n",
        "pretrained_vit.eval()\n",
        "with torch.inference_mode():\n",
        "    for X, y in test_dataloader:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        y_preds.append(torch.argmax(torch.softmax(pretrained_vit(X), dim=1), dim=1))\n",
        "\n",
        "y_preds = torch.cat(y_preds)\n",
        "len(y_preds)"
      ],
      "metadata": {
        "id": "zsgIWvIenfg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import time\n",
        "cm = confusion_matrix(y_preds.cpu(), new_test_dataloader.dataset.targets)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = class_names)\n",
        "\n",
        "cm_display.plot(cmap='Blues', colorbar=False, ax=ax, include_values=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E5Xwt-S8nEWn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}